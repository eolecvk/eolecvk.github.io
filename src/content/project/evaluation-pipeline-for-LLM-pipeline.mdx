---
tag:
  - llm
  - generative-ai
title: Behind the Scenes of an AI-Powered News Site
description: Trace, monitor and optimize an LLM chain in production
intro: Trace, monitor and optimize an LLM chain in production
features:
  - name: MLTimes
    value: https://news.lambda.ai
style:
  template: split
  card_template: grid
  hero_template: image
  hero_image_opacity: ""
  container: md
  block_class: my-0
thumbnail: /src/assets/langsmith-color.png
date: 2023-09-03T10:31:47.439Z
---

## Building a Trustworthy LLM Pipeline

**ML Times** is an AI-generated news site dedicated to summarizing and curating the most relevant updates in machine learning. Every day, it scans hundreds of sources, selects the most relevant stories, and produces concise summaries‚Äîall using a pipeline powered by large language models (LLMs).  

To ensure content quality, we didn‚Äôt just build a generation pipeline‚Äîwe also built a robust evaluation system to catch regressions, compare models and prompts, and keep iteration fast and cost-effective.  

This post shares how we built both the **LLM pipeline** and the **evaluation pipeline** that powers ML Times.


### üì∞ 1. From Raw Articles to Summarized News: The LLM Workflow

The ML Times pipeline processes content in the following stages:

#### **1. Article Ingestion**
We ingest a wide set of potential sources‚Äîfrom RSS feeds to blog crawlers. Not all articles are relevant to the ML community, so the next step is filtering.

#### **2. Relevance Classification**
An LLM is tasked with identifying whether a given article is actually relevant to ML practitioners. We experimented with both commercial and open-source models (e.g., GPT-3.5/4 and LLAMA 3.1) for this task.

**Key trick:** Rather than ask for a binary decision (relevant / not relevant), we asked for a **1‚Äì5 relevance score**, with carefully designed grading criteria. This let us:
- Set custom cutoff thresholds
- Gain nuance on borderline articles
- Avoid hallucinated justifications

#### **3. Summary Generation**
For relevant articles, we prompt LLMs to produce short summaries (~2‚Äì4 sentences) that highlight the key insight or development.

Over time, we found that **GPT-4** significantly outperformed GPT-3.5 and open-source models on this task‚Äîparticularly on:
- Capturing signal over fluff
- Avoiding hallucinations
- Writing fluid, readable summaries

We use **prompt-chaining** to include reasoning steps and justification fields that help with both human auditing and automated evaluation.


### üìè 2. Evaluation Pipeline: Measuring What Matters

We didn‚Äôt want to blindly trust LLM outputs, so we built an evaluation pipeline to track:
- Relevance classification accuracy
- Summary quality
- Chain performance across models and prompt versions

#### **Labelled Evaluation Dataset**
We manually labeled a representative subset of articles to define:
- Ground-truth relevance labels
- Examples of ‚Äúhigh-quality‚Äù summaries

This gave us a solid baseline to measure true/false positives/negatives.

#### **Prompt & Model Sweeps**
We evaluated the Cartesian product of:
- Models (GPT-3.5, GPT-4, LLAMA, Claude, etc.)
- Prompts (prompt variants for each task)

This let us compare options using the same benchmark set and iterate fast‚Äîespecially useful when optimizing prompts per model (e.g., GPT-4 often required different phrasing than GPT-3.5 for best results).

#### **Cost-Aware Design**
To keep costs down:
- We used an **open-source model** for the first-pass filter
- Used **justification fields** to audit errors before re-running more expensive models
- Limited evaluations to a high-quality benchmark set

#### **Trustworthiness Measures**
We built guardrails into the system:
- Evaluation chains output **justifications** alongside decisions
- Spot-checked outputs for consistency
- Compared model-graded results to human judgments
- Validated inter-model ranking stability

We used **LangSmith** for pipeline tracing, chain versioning, and prompt comparisons‚Äîthough in hindsight, we‚Äôd consider open-source options like **LangFuse** to avoid vendor lock-in.


### üìä 3. Insights & Outcomes

#### ‚úÖ Faster Iteration, Better Quality
- The evaluation system helped us catch issues that only surfaced in edge cases or long-tail articles.
- It gave us confidence to switch to GPT-4 for summarization, knowing the gain justified the cost.
- We avoided regressions by being able to **trace exactly where failures occurred** when experimenting.

#### ü§ñ Surprising Wins from Open Source
- We found that open-source models (e.g., LLAMA 3.1) performed well at **filtering non-relevant** articles.
- That enabled a tiered system: use the cheap model first, and fallback to GPT for borderline cases‚Äîcutting overall run cost.

#### üìâ Prompt Sensitivity
- Classification prompts tuned for GPT-3.5 sometimes performed worse when run on GPT-4‚Äîprompt compatibility is not always transferable.
- Summary performance improved more smoothly across models, but classification needed **model-specific tuning**.

#### üîç True Negative vs True Positive Tradeoffs
- Weaker models had high **true negative rates** but missed nuanced relevant stories.
- Stronger models (like GPT-4) were better at distinguishing between genuine insight and marketing fluff.


### üß± 4. Challenges & Decisions

#### üî¨ Building a Ground-Truth Dataset
- Required lots of manual work to label relevance accurately
- Sparked product discussions around what *should* be covered and how to define a ‚Äúquality summary‚Äù

#### üí∏ Experiment Budgeting
- Full sweeps across models √ó prompts √ó chains can get expensive
- We had to prioritize experiments likely to yield high signal

#### ‚öñÔ∏è Balancing Speed, Cost, and Quality
- We ran open-source models as first-pass filters
- Ran expensive evals only on shortlists or benchmarks
- Used explanations in outputs to guide prompt iterations


### ‚ú® 5. Looking Ahead

#### üõ†Ô∏è In Production
The evaluation setup is now part of our **daily workflow**:
- Summaries are continuously generated and filtered using the latest models
- Periodic evaluations ensure regressions are caught before going live
- Final summaries include traceable justification metadata

#### üß† What We‚Äôd Do Differently
- Explore open-source tools like **LangFuse** instead of LangSmith
- Use **multi-agent setups** to synthesize decisions for ambiguous articles (e.g., relevance via a vote or summary review)

#### üåç Beyond ML News?
- While ML Times is focused on one domain, this pipeline could easily be extended to:
  - Developer tooling changelogs
  - Financial news
  - Scientific research updates


### üßæ Conclusion

Building ML Times was not just about generating text‚Äîit was about building trust. The combination of:
- a thoughtful LLM pipeline,
- a strong evaluation backbone,
- and continuous improvement through model/prompt testing

...allowed us to deliver readable, relevant, and reliable machine learning news‚Äîpowered entirely by LLMs.

For anyone building LLM products at scale: don‚Äôt just build the output. Build the evaluators too.

---